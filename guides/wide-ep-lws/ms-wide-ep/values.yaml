multinode: true

modelArtifacts:
  uri: "hf://deepseek-ai/DeepSeek-R1-0528"
  size: 100Gi
  authSecretName: "llm-d-hf-token"
  name: "deepseek-ai/DeepSeek-R1-0528"

routing:
  modelName: deepseek-ai/DeepSeek-R1-0528
  servicePort: 8000
  parentRefs:
    - group: gateway.networking.k8s.io
      kind: Gateway
      # name: infra-wide-ep-inference-gateway # set in helmfile

  proxy:
    image: "ghcr.io/llm-d/llm-d-routing-sidecar:v0.2.0"
    secure: false
    connector: nixlv2

  inferenceModel:
    criticality: Critical
    create: true

  inferencePool:
    create: true
    # name: ms-wide-ep # set in helmfile

  httpRoute:
    create: true

  epp:
    create: true
    image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.2.1
    pluginsConfigFile: "pd-config.yaml"
    pluginsCustomConfig:
      pd-config.yaml: |
        # ALWAYS DO PD IN THIS EXAMPLE (THRESHOLD 0)
        apiVersion: inference.networking.x-k8s.io/v1alpha1
        kind: EndpointPickerConfig
        plugins:
        - type: prefill-header-handler
        - type: prefill-filter
        - type: decode-filter
        - type: max-score-picker
        - type: queue-scorer
          parameters:
            hashBlockSize: 5
            maxPrefixBlocksToMatch: 256
            lruCapacityPerServer: 31250
        - type: pd-profile-handler
          parameters:
            threshold: 0
            hashBlockSize: 5
        schedulingProfiles:
        - name: prefill
          plugins:
          - pluginRef: prefill-filter
          - pluginRef: queue-scorer
            weight: 1.0
          - pluginRef: max-score-picker
        - name: decode
          plugins:
          - pluginRef: decode-filter
          - pluginRef: queue-scorer
            weight: 1.0
          - pluginRef: max-score-picker
decode:
  create: true
  replicas: 1
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200
  parallelism:
    # TODO: The value for parallelism.data is a hack to get the pod-per-node case working.
    # This must equal the number of nodes rather than the dp_size.
    data: 2
    tensor: 1 # these will be derived based performance testing
  monitoring:
    podmonitor:
      enabled: false
      portName: "metrics" # decode vLLM service port (from routing.proxy.targetPort)
      path: "/metrics"
      interval: "30s"
  containers:
    - name: vllm-worker-decode
      image: "ghcr.io/llm-d/llm-d:v0.2.0"
      imagePullPolicy: Always
      workingDir: /code
      command: ["/bin/bash", "-c"]
      modelCommand: "custom"
      readinessProbe:
        httpGet:
          path: /health
          port: 8200
      args:
        - |-
          #################
          # RUN vLLM decode worker
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

          source /opt/vllm/bin/activate
          exec vllm serve \
            deepseek-ai/DeepSeek-R1-0528 \
            --port 8200 \
            --disable-log-requests \
            --disable-uvicorn-access-log \
            --enable-expert-parallel \
            --data-parallel-hybrid-lb \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
            --data-parallel-size-local $DP_SIZE_LOCAL \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-start-rank $START_RANK \
            --trust-remote-code \
            --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'

      ports:
        - containerPort: 8200
          name: metrics
          protocol: TCP
      env:
        - name: VLLM_FUSED_MOE_CHUNK_SIZE
          value: "1024"

        - name: DP_SIZE_LOCAL
          value: "8"
        #- name: UCX_TLS
        #  value: "rc,sm,self"
        #- name: UCX_PROTO_INFO
        #  value: "y"
        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_low_latency"
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: HF_HUB_CACHE
          value: /huggingface-cache
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
      securityContext:
        runAsUser: 0
        runAsGroup: 0
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "8"
          rdma/ib: 1
        requests:
          cpu: 32
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "8"
          rdma/ib: 1
      mountModelVolume: false
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: hf-cache
          mountPath: /huggingface-cache

  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
    - name: hf-cache
      hostPath:
        path: /mnt/local/hf-cache
        type: DirectoryOrCreate

prefill:
  create: true
  replicas: 1
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200
  parallelism:
    # TODO: The value for parallelism.data is a hack to get the pod-per-node case working.
    # It must equal the number of nodes rather than the dp_size.
    data: 1
    tensor: 1 # these will be derived based performance testing
  monitoring:
    podmonitor:
      enabled: false
      portName: "metrics" # prefill vLLM service port (from routing.servicePort)
      path: "/metrics"
      interval: "30s"
  containers:
    - name: vllm-worker-prefill
      image: "ghcr.io/llm-d/llm-d:v0.2.0"
      imagePullPolicy: Always
      workingDir: /code
      command: ["/bin/bash", "-c"]
      modelCommand: "custom"
      readinessProbe:
        httpGet:
          path: /health
          port: 8000
      args:
        - |-
          #################
          # RUN vLLM prefill worker
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

          source /opt/vllm/bin/activate
          exec vllm serve \
            deepseek-ai/DeepSeek-R1-0528 \
            --port 8000 \
            --disable-log-requests \
            --disable-uvicorn-access-log \
            --enable-expert-parallel \
            --data-parallel-hybrid-lb \
            --tensor-parallel-size $TP_SIZE \
            --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
            --data-parallel-size-local $DP_SIZE_LOCAL \
            --data-parallel-address ${LWS_LEADER_ADDRESS} \
            --data-parallel-rpc-port 5555 \
            --data-parallel-start-rank $START_RANK \
            --trust-remote-code \
            --kv_transfer_config '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'

      ports:
        - containerPort: 8000
          name: metrics
          protocol: TCP
      env:
        - name: DP_SIZE_LOCAL
          value: "8"

        #- name: UCX_TLS
        #  value: "rc,sm,self"
        #- name: UCX_PROTO_INFO
        #  value: "y"

        - name: TRITON_LIBCUDA_PATH
          value: "/usr/lib64"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        - name: VLLM_SKIP_P2P_CHECK
          value: "1"
        - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
          value: "1"
        - name: VLLM_USE_DEEP_GEMM
          value: "1"
        - name: VLLM_ALL2ALL_BACKEND
          value: "deepep_high_throughput"
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "INFO"
        - name: HF_HUB_CACHE
          value: /huggingface-cache
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
      securityContext:
        runAsUser: 0
        runAsGroup: 0
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "8"
          rdma/ib: 1
        requests:
          cpu: 32
          memory: 512Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: "8"
          rdma/ib: 1
      mountModelVolume: false
      volumeMounts:
        - name: dshm
          mountPath: /dev/shm
        - name: hf-cache
          mountPath: /huggingface-cache

  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
    - name: hf-cache
      hostPath:
        path: /mnt/local/hf-cache
        type: DirectoryOrCreate
